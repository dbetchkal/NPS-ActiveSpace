# -*- coding: utf-8 -*-
"""
Function library for ActiveSpace creation and truthing.

Created 2021-08-04 09:57

@author: Kirby Heck

"""

# geospatial libraries
import pyproj
import geopandas as gpd
import gdal
from gdalconst import GA_ReadOnly
from shapely.geometry import Point, Polygon
import rasterio
import rasterio.plot
import rasterio.mask
from rasterio.windows import Window

# other libraries
import matplotlib as mpl
import matplotlib.pyplot as plt
from matplotlib.tri import Triangulation
from matplotlib.widgets import RangeSlider, Button, Slider, RadioButtons

import numpy as np
from scipy import interpolate
import pandas as pd
import pickle
import glob
import os
import subprocess
from itertools import islice
import datetime as dt
import tempfile
import re
import warnings


def get_utm_zone(project_dir):
    """
    Glean the UTM zone that was saved as a suffix to the elevation file's name. Deprecated, see coord_to_UTM() or
    get_site_info()

    This assumes that the elevation file was clipped using the NSNSD ArcToolbox
    packaged with this tool.
    """

    # the ArcPro tool will figure out the UTM zone associated with the western extent of the file
    # (this is the native UTM zone of the NMSIM project)
    elev_filename = os.path.basename(glob.glob(project_dir + os.sep + r"Input_Data\01_ELEVATION\*.flt")[0])

    # this will glean the UTM zone from the filename as an integer
    UTM_zone = str(elev_filename.split("_")[-1][3:-4])

    return UTM_zone


def coord_to_UTM(coord, crs=None):
    """
    Takes a coordinate pair and outputs an epsg code corresponding to the UTM zone of the point.

    Parameters
    ----------
    coord (tuple OR shapely point): (long, lat) coordinate in the given projection OR WGS84 (default)
    crs (str): optional, if not None, assumes the given coordinate is in that coordinate projection

    Returns
    -------
    utm_zone (str): UTM zone name (e.g. 'epsg:26905' for UTM 5N)
    """

    try:
        coord = (coord.x, coord.y)
    except:
        pass  # coordinate is a tuple, not a shapely point

    if crs is not None:  # reproject into WGS84 from given crs
        pt = gpd.GeoDataFrame(geometry=[Point(coord)], crs=crs)
        pt = pt.to_crs('epsg:4326')  # convert to WGS84
        coord = (pt.geometry.values.x[0], pt.geometry.values.y[0])  # overwrite the old point with WGS84 coordinates

    zone = int((coord[0] + 180) // 6 + 1)  # 6 degrees per zone; add 180 because zone 1 starts at 180 W

    if coord[1] > 0:
        utm_zone = 'epsg:269{:02d}'.format(zone)  # northern hemisphere
    else:
        utm_zone = 'epsg:327{:02d}'.format(zone)  # southern hemisphere

    return utm_zone


def climb_angle(v):
    """
    Compute the 'climb angle' of a vector
    A = ð‘›â€¢ð‘=|ð‘›||ð‘|ð‘ ð‘–ð‘›(ðœƒ)
    """

    # a unit normal vector perpendicular to the xy plane
    n = np.array([0, 0, 1])

    degrees = np.degrees(np.arcsin(np.dot(n, v) / (np.linalg.norm(n) * np.linalg.norm(v))))

    return degrees


def get_extent(project_dir):
    """
    The `NMSIM_Create_Base_Layers` ArcToolbox will create a .tif elevation raster.
    Use that to quickly determine the project's geographic extent.

    Parameters
    ----------
    project_dir (str): project directory to look for an elevation file

    Returns
    -------
    Array of boundary values ([minx, maxx], [miny, maxy]) in UTM coordinates
    UTM zone (string)
    """

    # find the UTM zone from the elevation (.flt) file generated by `NMSIM Create Base Layers`
    zone = get_utm_zone(project_dir)

    # epsg codes for UTM zones in the United States (haven't tested Hawaii yet!)
    epsg_lookup = {"1": 'epsg:26901', "2": 'epsg:26902', "3": 'epsg:26903', "4": 'epsg:26904', "5": 'epsg:26905',
                   "6": 'epsg:26906', "7": 'epsg:26907', "8": 'epsg:26908', "9": 'epsg:26909', "10": 'epsg:26910',
                   "11": 'epsg:26911', "12": 'epsg:26912', "13": 'epsg:26913', "14": 'epsg:26914', "15": 'epsg:26915',
                   "16": 'epsg:26916', "17": 'epsg:26917'}

    # find the elevation .tif and open it read only
    full_dir = project_dir + os.sep + r"Input_Data\01_ELEVATION"
    tif_path = glob.glob(full_dir + os.sep + "*.tif")[0]
    data = gdal.Open(tif_path, GA_ReadOnly)

    # extract the (projected) geometric transformation of the NAD83 raster
    # glean each of the corners
    geoTransform = data.GetGeoTransform()
    minx = geoTransform[0]
    maxy = geoTransform[3]
    maxx = minx + geoTransform[1] * data.RasterXSize
    miny = maxy + geoTransform[5] * data.RasterYSize

    UTM_zone = epsg_lookup[zone]

    transformer = pyproj.Transformer.from_crs('epsg:4269', UTM_zone, always_xy=True)
    out_points = transformer.transform([minx, maxx], [miny, maxy])

    return out_points, UTM_zone


def NMSIM_create_tis(project_dir, source_path, Nnumber=None, NMSIMpath=None, printout=False):
    """
    Create a site-based model run (.tis) using the NMSIM batch processor.

    Inputs
    ------
    project_dir (str, path): the location of a canonical NMSIM project directory, created with "Create_Base_Layers.py"
    source_path (str, path): the location of the relevant NMSIM noise source file (.src)
    NMSIMpath (str, path): [optional] an alternate location of the program `Nord2000batch.exe`
    printout (bool): [optional] additional status updates to the console

    Returns
    -------
    None

    """

    # ======= (1) define obvious, one-to-one project files ================

    elev_file = glob.glob(project_dir + os.sep + r"Input_Data\01_ELEVATION\elevation_m*.flt")[0]

    # imped_file = project_dir + os.sep + "Input_Data\01_IMPEDANCE" + os.sep + "landcover.flt"
    imped_file = None

    trj_files = glob.glob(project_dir + os.sep + r"Input_Data\03_TRAJECTORY\*.trj")

    # eventually the batch file is going to want this
    tis_out_dir = project_dir + os.sep + r"Output_Data\TIG_TIS"

    # ======= (2) define less obvious project files - these still need thought! ================

    # site files need some thinking through... there COULD be more than one per study area
    # (it's quite project dependant)
    site_file = glob.glob(project_dir + os.sep + r"Input_Data\05_SITES\*.sit")[0]

    # strip out the FAA registration number
    registrations = [t.split("_")[-3][11:] for t in trj_files]

    # the .tis name preserves: reciever + source + time (roughly 'source : path : reciever')
    site_prefix = os.path.basename(site_file)[:-4]

    tis_files = [tis_out_dir + os.sep + site_prefix + "_" + os.path.basename(t)[:-4] for t in trj_files]

    trajectories = pd.DataFrame([registrations, trj_files, tis_files], index=["N_Number", "TRJ_Path", "TIS_Path"]).T

    # ======= (3) write the control + batch files for command line control of NMSIM ================

    # set up the two files we want to write
    control_file = project_dir + os.sep + "control.nms"
    batch_file = project_dir + os.sep + "batch.txt"

    # select the trajectories to process
    if (Nnumber == None):

        trj_to_process = trajectories

    else:

        trj_to_process = trajectories.loc[trajectories["N_Number"] == Nnumber, :]

    if (NMSIMpath == None):

        # NMSIM is packaged right along with these scripts
        # so by default we look for `Nord2000batch.exe` relative to this file
        this_file = os.path.abspath(__file__)
        one_dir_up = os.path.dirname(this_file)

        Nord = one_dir_up + os.sep + r"NMSIM\Nord2000batch.exe"

    else:

        Nord = NMSIMpath

    for meta, flight in trj_to_process.iterrows():

        # write the control file for this situation
        with open(control_file, 'w') as nms:

            nms.write(elev_file + "\n")  # elevation path

            if (imped_file != None):
                nms.write(imped_file + "\n")  # impedance path
            else:
                nms.write("-\n")

            nms.write(site_file + "\n")  # site path
            nms.write(flight["TRJ_Path"] + "\n")
            nms.write("-\n")
            nms.write("-\n")
            nms.write(source_path + "\n")
            nms.write("{0:11.4f}   \n".format(500.0000))
            nms.write("-\n")
            nms.write("-")

            # write the batch file to create a site-based analysis
        with open(batch_file, 'w') as batch:

            batch.write("open\n")
            batch.write(control_file + "\n")
            batch.write("site\n")
            batch.write(flight["TIS_Path"] + "\n")
            batch.write("dbf: no\n")
            batch.write("hrs: 0\n")
            batch.write("min: 0\n")
            batch.write("sec: 0.0")

        # ======= (4) compute the theoretically observed trace on the site's microphone ================

        print(flight["TRJ_Path"] + "\n")

        process = subprocess.Popen([Nord, batch_file], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        stdout, stderr = process.communicate()

        output_messages = stdout.decode("utf-8").split("\r\n")
        output_messages = [out for out in output_messages if out.strip() != '']

        if printout:
            print("\tthe following lines are directly from NMSIM:")
            for s in output_messages + ["\n"]:
                print("\t" + s)

        # slightly messier printing for error messages
        if (stderr != None):
            for s in stderr.decode("utf-8").split("\r\n"):
                print(s.strip())


def generate_tracks(project_dir, n_tracks, density=48, extent=None, printout=False, trajectory=None):
    """
    Generates artificial flight tracks to be formatted and written later into .trj trajectories.

    Inputs
    ------
    project_dir (str): project directory (ends with 4-letter unit code plus 4-letter site code)
    n_tracks (int): number of tracks to create
    density (int): (default 48) number of points per grid dimension (e.g. will generate density^2 points)
    extent (tuple): (default from get_extent) extent, formatted as ([xmin, xmax], [ymin, ymax]) for
        the bounds of the rectangle
    printout (bool): (default False) prints status updates to console
    trajectory ([Nx2] numpy array): (default None) uses a prescribed set of points instead of creating a grid with the
        given extents as the trajectory points.

    Outputs
    -------
    tracks (DataFrame): tracks to write trajectories for, formatted as
        index (0, 1, ...)       Trajectory              Timestamps (antequated...)
        0                       [x_pos, y_pos]          0
    """

    if printout:
        print(f"generate_tracks: Generating {n_tracks} tracks with density {density}")

    # glean the bounds of the project in UTM Coordinates
    if extent is None:
        extent, utm_zone = get_extent(project_dir)
    else:
        # do not overwrite supplied extent
        _, utm_zone = get_extent(project_dir)

    tracks = pd.DataFrame([], columns=["Trajectory", "Timestamps"])

    if trajectory is None:
        # if the project is rectangular, the extent will give a rough sense of width and height
        width = extent[0][1] - extent[0][0]
        height = extent[1][1] - extent[1][0]

        if printout:
            print("width", width, "height", height)

        # new tracks method: start with a grid
        # start out with a grid of N = density^2 points
        x = np.linspace(extent[0][0], extent[0][1], density)
        y = np.linspace(extent[1][0], extent[1][1], density)
        x_ind, y_ind = np.meshgrid(x, y)

        trajectory = np.array([np.ravel(x_ind), np.ravel(y_ind)]).T  # np.ravel linearly indexes an array into a row

    for i in range(n_tracks):
        # this is sloppy, but quick. A bit of a memory hog. Better options?
        # duplicates the created `trajectory` into a list n_tracks long (all the same)
        tracks = tracks.append({"Trajectory": trajectory, "Timestamps": np.zeros(density ** 2)}, ignore_index=True)

    return tracks, utm_zone


def create_trajectories(project_dir, year, n_tracks, altitude, velocity,
                        coord=None, crs=None, id_string="TrackNum_",
                        density=48, extent=None, heading="set", xy=None, printout=False, reuse_old=False):
    """
    Calls generate_tracks or takes a tracks input dataframe. Creates trajectory files for each entry
    in the tracks dataframe (see generate_tracks for formatting details) that correspond with an NPS
    site in the trajectories folder.

    Inputs
    ------
    project_dir (str): project directory (ends with 4-letter unit code plus 4-letter site code)
    year (int): year of site visit
    n_tracks (int): number of tracks to create
    altitude (float): flight altitude (constant), in [m]
    velocity (float): flight velocity (constant), in [m/s] (check: does this matter?)
    coord (tuple): (long, lat) coordinate in `crs` OR WGS84 (default). Coordinates are used to generate the site file
        if the project directory is not a microphone site. Default None.
    crs (str): optional, if not None, assumes the given coordinate is in that coordinate projection
    density (int): (default 48) number of points per grid dimension, if using generate_tracks
    id_string (str): (default "TrackNum_") trajectory file name
    extent (tuple): (default from get_extent) extent, formatted as ([xmin, xmax], [ymin, ymax])
        to pass to generate_tracks
    heading (str): (default "set") either "set" (non-random, uniform headings), "rand" (random, non-uniform), or
        "calc" (calculate based on trajectory path)
    xy ([Nx2] array): (default None) points to write to trajectories, passed to generate_tracks
    printout (bool): (default False) prints status updates to console
    reuse_old (bool): (default False) skips deleting old tracks and appends new tracks instead.

    Outputs
    -------
    track (GeoDataFrame): final track written to file

    Throws
    ------
    IndexError if the created trajectory is empty (e.g. xy is passed in as an empty list)
    """

    # only generate tracks if the user does not input any
    if printout:
        print("generating", n_tracks, "tracks")

    trajectory_raw, utm_zone = generate_tracks(project_dir, n_tracks=n_tracks, density=density,
                                               extent=extent, printout=printout, trajectory=xy)

    # ================== generate NMSim site file ========================

    if printout:
        print("generating NMSIM .sit file")

    # UNITSITEYYYY format; this assumes the project directory is named for the site!
    designator = os.path.basename(project_dir) + str(year)

    try:  # if the site is an inventory site, look it up
        u, s, yr = parse_designator(designator)
        site_info = get_site_info(unit_name=u, site_name=s, year=yr)  # in [m]
        long_utm = site_info.x.values[0]
        lat_utm = site_info.y.values[0]
        mic_height = site_info.microphone_height.values[0]

    except ValueError:  # site does not exist
        zone = coord_to_UTM(coord, crs=crs)
        pt = gpd.GeoDataFrame(geometry=[Point(coord)], crs=crs)
        pt.to_crs(zone, inplace=True)
        long_utm, lat_utm = pt.geometry.values.x[0], pt.geometry.values.y[0]
        mic_height = 1.60  # m, average height of human ear

    # NOTE: THIS WILL ONLY WORK FOR SITES IN THE FORMAT UNITSITEYYYY
    create_NMSIM_site_file(project_dir, designator[0:4], designator[4:8], long_utm, lat_utm, mic_height)

    # ==================== generating flight trajectories =========================

    print("create_trajectories: Writing new flight trajectories for NMSim. ")
    # pads the index i when writing files if not overwriting old trajectories
    num_pad = 0
    trj_dir = project_dir + os.sep + r"Input_Data\03_TRAJECTORY"
    tis_dir = project_dir + os.sep + r"Output_Data\TIG_TIS"

    # first step is to clean out old trajectories
    if not reuse_old:
        for f in os.listdir(trj_dir):
            if f.endswith(".trj"):
                os.remove(os.path.join(trj_dir, f))

        for f in os.listdir(tis_dir):  # these should match... but no guarantees!
            if f.endswith(".tis"):
                os.remove(os.path.join(tis_dir, f))
    else:
        num_pad = len(os.listdir(trj_dir))

    # ================== iterate through each trajectory and save ======================

    for i, row in trajectory_raw.iterrows():

        if printout:
            print("computing trajectory", i + 1 + num_pad)

        # grab the points in UTM and save ourselves a lot of time
        points = trajectory_raw.loc[i, "Trajectory"].T
        long = points[0]
        lat = points[1]

        # set up the GeoDataFrame
        track = gpd.GeoDataFrame([],
                                 geometry=[Point(w[0], w[1]) for w in trajectory_raw.loc[i, "Trajectory"]],
                                 crs=utm_zone)

        track["lat_UTM"] = lat
        track["long_UTM"] = long
        track["elev_MSL_m"] = np.full(track["lat_UTM"].shape, altitude, dtype=None, order='C')

        # convert the p(x, y, z) data into an array of vectors
        V = np.diff(np.array([[x, y, z] for x, y, z in zip(track['long_UTM'],
                                                           track['lat_UTM'],
                                                           track['elev_MSL_m'])]), axis=0)

        # headings using the x and y components of each vector
        if heading == "set":
            # evenly space out the headings based on how many tracks are generated
            headings = np.full(track["lat_UTM"].shape, i / n_tracks * 360)
        elif heading == "rand":
            # `randomly` assign headings for each point
            RNG = np.random.default_rng()
            headings = RNG.random(size=track["lat_UTM"].shape[0]) * 360
        else:
            # calculate the heading based on the flight path trajectory
            if heading != "calc":
                # show a warning if the user requested none of the valid options...
                warnings.warn(
                    "Invalid parameter for 'heading' in create_trajectories(). Calculating from tracks...")

            headings = np.degrees(np.arctan2(V.T[0], V.T[1]))
            headings = np.append(headings, np.nan)  # need one row on the end

        # climb angle using the vector and the unit normal in the xy plane; add np.nan to the end
        climb_angles = np.array([climb_angle(v) for v in V])
        climb_angles = np.append(climb_angles, np.nan)

        # compute the time elapsed given cruising velocity of the aircraft in question; add np.nan to the end
        time_elapsed = np.cumsum(np.array([np.linalg.norm(v) for v in V]) / velocity)
        time_elapsed = np.append(time_elapsed, np.nan)

        # convert airspeed to knots and write it to the geodataframe
        track["knots"] = np.full(track["lat_UTM"].shape, 1.94384 * velocity, dtype=None, order='C')

        # write the remaining data to the geodataframe
        track["climb_angle"] = climb_angles
        track["time_elapsed"] = time_elapsed
        track["heading"] = headings

        # path to the specific .trj file to be written, pad numbers if appending to existing tracks
        trj_path = project_dir + os.sep + r"Input_Data\03_TRAJECTORY" + os.sep + id_string + str(
            i + 1 + num_pad) + ".trj"

        if printout:
            print("writing", trj_path)

        with open(trj_path, 'w') as trajectory:

            # write the header information [exactly as NMSIM expects!]
            trajectory.write("Flight track trajectory variable description:\n")
            trajectory.write(" time - time in seconds from the reference time\n")
            trajectory.write(" Xpos - x coordinate (UTM)\n")
            trajectory.write(" Ypos - y coordinate (UTM)\n")
            trajectory.write(" UTM Zone  " + str(utm_zone) + "\n")
            trajectory.write(" Zpos - z coordinate in meters MSL\n")
            trajectory.write(" heading - aircraft compass bearing in degrees\n")
            trajectory.write(" climbANG - aircraft climb angle in degrees\n")
            trajectory.write(" vel - aircraft velocity in knots\n")
            trajectory.write(" power - % engine power\n")
            trajectory.write(" roll - bank angle (right wing down), degrees\n")
            trajectory.write("FLIGHT " + id_string + "\n")
            trajectory.write("TEMP.  59.0\n")
            trajectory.write("Humid.  70.0\n")
            trajectory.write("\n")
            trajectory.write(
                "         time(s)        Xpos           Ypos           Zpos         heading        climbANG       Vel            power          rol\n")

            # now write the data section row by row
            # (except for the last row - it's null due to the differencing)
            # for ind, point in track.iloc[:-1].iterrows():

            # just kidding, make all the rows the same (including the last row)
            for ind, point in track.iterrows():
                # write the line
                trajectory.write("{0:15.3f}".format(point["time_elapsed"]) + \
                                 "{0:15.3f}".format(point["long_UTM"]) + \
                                 "{0:15.3f}".format(point["lat_UTM"]) + \
                                 "{0:15.3f}".format(point["elev_MSL_m"]) + \
                                 "{0:15.3f}".format(point["heading"]) + \
                                 "{0:15.3f}".format(point["climb_angle"]) + \
                                 "{0:15.3f}".format(point["knots"]) + \
                                 "{0:15.3f}".format(95) + \
                                 "{0:15.3f}".format(0) + "\n")

            if printout:
                print("\t\t\t...finished writing .trj", "\n")

    # note: this will just return the last track computed
    return track


def get_contour_xy(total_space, max_pts=5184):
    """
    Takes a DataFrame of audible and inaudible points and creates a long list of [x,y] UTM coordinates
    that fall within the (0,1) contours.

    Inputs
    ------
    total_space (gdf): Combined active and inactive space as a Geodataframe with an 'audible' column denoting if that
        point can be heard
    max_pts (int): optional, maximum number of points to triangulate lest NMSim may run out of memory

    Outputs
    -------
    xy (array): [Nx2] array of points that lie on the contours between 0 (inaudible) and 1 (audible) based on the
        Triangulated Irregular Network (free-form contour) of prior points where N â‰¤ max_pts.
    """

    # =============== triangulation ==============
    total_space.sort_index(inplace=True)  # this may be unnecessary
    contour_data = total_space.values[:, :-1]
    fig, ax = plt.subplots()

    levels = np.linspace(0, 1, 10, endpoint=False)  # these are the contour levels to calculate
    x_tri = contour_data[:, 0]  # triangulation points
    y_tri = contour_data[:, 1]
    z_tri = contour_data[:, 2]

    tri = mpl.tri.Triangulation(list(x_tri), list(y_tri))  # uses Delaunay triangulation

    cs = ax.tricontour(tri, list(z_tri), levels=levels)  # contour with arbitrary point cloud
    ax.triplot(tri, color='0.7', lw=1)

    # append all contour points to a new xy file
    xy = np.array([])
    for contour_level in cs.collections:  # iterate through each contour interval
        for contour_path in contour_level.get_paths():  # for each interval, iterate through each path
            x = contour_path.vertices[:, 0]
            y = contour_path.vertices[:, 1]

            if len(xy) == 0:
                xy = np.array([x, y]).T
            else:
                xy = np.append(xy, np.array([x, y]).T, axis=0)

    # if we have more contour points than NMSim can handle, down-sample randomly
    if xy.shape[0] > max_pts:
        random_inds = np.random.randint(0, xy.shape[0], max_pts)
        xy = xy[random_inds, :]

    return xy


def parse_designator(des):
    """"
    Parses a 12-character string designator (UNITSITEYYYY) into its components.

    TODO: parse non-12 character site designators

    Parameters
    ----------
    des (str): Designator string (12 characters) formatted UNITSITEYYYY OR UNITSITE (8 characters)

    Returns
    -------
    unit, site, yr: unit (str), site (str), year (int)

    Throws
    ------
    ValueError if the designator string is not length 8 or 12
    """

    if not (len(des) == 12 or len(des) == 8):
        raise ValueError("Designator string is not 12 characters long")
    u = des[:4]
    s = des[4:8]

    if len(des) == 8:
        return u, s, None

    y = des[-4:]
    return u, s, int(y)


def get_site_info(unit_name='DENA', site_name=None, year=None, coord=None, crs=None, elev=None, utm_zone=None,
                  metadata_path=r"V:\Complete_Metadata_AKR_2001-2021.txt",
                  DEM_path=r"T:\ResMgmt\WAGS\Sound\Users\Kirby_Heck\NIMSIM_DEMSs\16_Bit\DENA_DEM_m_4269.TIF"):
    """
    Parses metadata from the `V:\` drive to find the lat/long/elevation of a site for a given designator (UNITSITEYYYY)

    Inputs
    ------
    unit_name (str): optional, four character park code ('DENA'). Default DENA if none provided
    site_name (str): optional, four character site name ('KAHP'). Default `None`
    yr (int): optional, default `None` (if none provided, will look up most recent visit)
    coord (tuple): optional, coordinates of the site (long, lat) or (x, y)
    crs (str): required for reading in long/lat coordinates. Formatted like 'epsg:4326' for WGS84
    elev (float): optional, elevation of coordinate [meters]
    utm_zone (str): optional, coordinate reference frame for the given region (e.g. 'epsg:26905')
    metdata_path (str): `V:\` drive path

    Returns
    -------
    Pandas dataframe with:
    x, y, utm_zone, long, lat, elevation, [rest of metadata] of the microphone. x, y, and elevation are all in meters

    Throws
    ------
    ValueError if the site designator does not follow the format 'UNITSITEYYYY'
    """

    if coord is not None and crs is not None:
        # build a site_info dataframe
        utm_zone = coord_to_UTM(coord, crs)
        pt = gpd.GeoDataFrame(geometry=[Point(coord)], crs=crs)  # this should probably be a GeoSeries

        # if we're already in UTM, then x, y are done
        if utm_zone == crs:
            x = coord[0]
            y = coord[1]

        else:
            pt.to_crs(utm_zone, inplace=True)
            x = pt.geometry.x.values[0]
            y = pt.geometry.y.values[0]

        # next, get the elevation if none is provided
        if elev is None:
            with rasterio.open(DEM_path) as dem:
                pt.to_crs(dem.crs, inplace=True)
                x_dem = pt.geometry.x.values[0]
                y_dem = pt.geometry.y.values[0]

                band1 = dem.read(1)  # this unfortunately takes a little bit
                elev = band1[dem.index(x_dem, y_dem)]

        # get latitude and longitude for good measure
        pt.to_crs('epsg:4326', inplace=True)  # this is a lot of projection gymnastics, ugh
        long = pt.geometry.x.values[0]
        lat = pt.geometry.y.values[0]

        # create the site_info dataframe
        site_info = pd.DataFrame({'lat': [lat],
                                  'long': [long],
                                  'elevation': [elev],
                                  'x': [x],
                                  'y': [y],
                                  'utm_zone': [utm_zone]})
        return site_info

    elif unit_name is not None and site_name is not None:
        # load the metadata sheet
        metadata = pd.read_csv(metadata_path, delimiter="\t", encoding="ISO-8859-1")

        if year is None:
            year = 2019  # throw a dummy year in here

        # look up the site's coordinates in WGS84
        try:
            site_meta = metadata.loc[(metadata["unit"] == unit_name) &
                                     (metadata["code"] == site_name) &
                                     (metadata["year"] == year)]
            site_meta.iloc[0]  # check to see if there are data

        except IndexError:
            warnings.warn("No match for " + unit_name+site_name+str(year) + ", looking at other years.")
            site_meta = metadata.loc[(metadata["unit"] == unit_name) &
                                     (metadata["code"] == site_name)]
            warnings.warn("Found {} sites for {}, most recent visit in {}.".format(len(site_meta),
                                                                                   unit_name + site_name,
                                                                                   (site_meta.year).max()))
            site_meta = site_meta.loc[site_meta.year == (site_meta.year).max()]

        lat_in, long_in = site_meta.loc[:, ["lat", "long"]].values[0]  # Throws IndexError if there are no years of data

        if utm_zone is None:  # calculate the UTM zone and return it
            utm_zone = coord_to_UTM((long_in, lat_in))

        # convert from D.d (WGS84) to meters (NAD83)
        in_proj = pyproj.CRS.from_string('epsg:4326')
        out_proj = pyproj.CRS.from_string(utm_zone)

        # convert coordinate system
        WGS84_to_UTM = pyproj.Transformer.from_crs(in_proj, out_proj)
        x, y = WGS84_to_UTM.transform(lat_in, long_in)  # apply projection

        # append UTM coordinates to the metadata
        site_meta = site_meta.assign(x=x, y=y, utm_zone=utm_zone)

    else:  # insufficent information was provided
        raise ValueError('Either a coordinate system with long/lat coordinates '
                         'or a unit and site name (and year, opt.) must be given. ')

    return site_meta


def create_NMSIM_site_file(project_dir, unit, site, long_utm, lat_utm, height):
    """
    Create an NMSIM site file for a given NPS monitoring deployment.

    Inputs
    ------
    project_dir (str): a canonical NMSIM project directory
    unit (str): 4-character NPS Alpha Code, e.g. "BITH", "YUCH"
    site (str): alpha-numeric acoustic monitoring site code, e.g., "002", "TRLA"
    long_utm (float): longitude in meters for the NMSIM project's UTM zone
    lat_utm (float): latitude in meters for the NMSIM project's UTM zone
    height (float): microphone height in meters

    Returns
    -------
    None (creates files in the project directory)
    """

    # the full path to the eventual NMSIM site file
    out_path = project_dir + os.sep + r"Input_Data\05_SITES" + os.sep + unit + site + ".sit"
    # out_path = project_dir + os.sep + r"Input_Data\05_SITES" + os.sep + 'NONSENSE.sit'

    # open a file and write to it
    with open(out_path, 'w') as site_file:
        site_file.write("    0\n")
        site_file.write("    1\n")
        site_file.write("{0:19.0f}.{1:9.0f}.{2:10.5f} {3:20}\n".format(long_utm, lat_utm, height, unit + site))
        site_file.write(glob.glob(project_dir + os.sep + r"Input_Data\01_ELEVATION\*.flt")[0] + "\n")


def tis_resampler(tis_path, utc_offset=-8):
    """
    Reads in a tis file from NMSIM and returns a more beautiful pandas dataframe.

    NOTE: This is NOT the same tis_resmpler() script as in NMSIM_Create_Base_Layers.py
    """

    # read the data line-by-line
    with open(tis_path) as f:
        content = list(islice(f, 18 + (3600 * 24)))

        # find the line index where the header ends
    splitBegin = content.index('---End File Header---\n')

    # take out the whitespace and two empty columns at either end
    spectral_data = [re.split(r'\s+', c.lstrip()) for c in content[splitBegin + 10:]]
    spectral_data = [d[:-2] for d in spectral_data]

    # initalize a pandas dataframe using the raw spectral data and the expected column headers
    tis = pd.DataFrame(spectral_data,
                       columns=["SP#", "TIME", "F", "A", "10", "12.5", "15.8", "20", "25", "31.5", "40", "50", "63",
                                "80", "100", "125", "160", "200", "250", "315", "400", "500", "630", "800", "1000",
                                "1250", "1600", "2000", "2500", "3150", "4000", "5000", "6300", "8000", "10000",
                                "12500"], dtype='float')

    # there's a weird text line at the end of the file (is this true for all .tis files?) >> yes
    tis.drop(tis.tail(1).index, inplace=True)  # drop last n rows

    # these columns are stubborn
    tis["TIME"] = tis["TIME"].astype('float')
    tis["SP#"] = tis["SP#"].astype('float').apply(lambda f: int(f))
    tis["F"] = tis["F"].astype('int')

    # convert relevant columns to decibels (dB) from centibels (cB)
    tis.loc[:, 'A':'12500'] = tis.loc[:, 'A':'12500'].astype(float) * 0.1  # this one is stubborn too

    return tis


def pair_trj_to_tis_results(project_dir):
    """
    Join a directory of .tis results created by NMSIM to the .trj files that created them.

    Inputs
    ------
    project_dir (str): the path to a canonical NPS-style NMSIM project directory

    Returns
    -------
    iterator (zip object): an iterator containing the paired .tis and .trj file paths

    """

    # find all the '.tis' files
    successful_tis = glob.glob(project_dir + os.sep + "Output_Data\TIG_TIS\*.tis")

    # find all the '.trj' files
    trajectories = [project_dir + os.sep + "Input_Data\\03_TRAJECTORY" + \
                    os.sep + os.path.basename(f)[9:-4] + ".trj" for f in successful_tis]

    iterator = zip(trajectories, successful_tis)

    return iterator


def trj_reader(trj_path):
    """
    Parses a trajectory file and returns the data in a pandas dataframe.
    """

    # Using readlines()
    file1 = open(trj_path, 'r')
    lines = file1.readlines()

    # glean the header information
    header = lines[15]
    header = header.strip()

    # clean up the header for use as column names in the DataFrame
    headersplit = header.split()
    while "" in headersplit:
        headersplit.remove("")

    # break each line apart into
    splitlines = [l.strip().split(" ") for l in lines[16:]]

    outlines = []

    for splitline in splitlines:
        while "" in splitline:
            splitline.remove("")

        outlines.append(splitline)

    out = pd.DataFrame(outlines, columns=headersplit)

    return out


def interp_spline(flight_utm, starttime, endtime, dT=1., out_crs="epsg:4326", plotting=False,
                  time_index='ak_datetime'):
    """
    Interpolates points with a cubic spline between flight points, if possible
    See https://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html#spline-interpolation for docs

    Inputs
    ------
    flight_utm (geodataframe): a Pandas dataframe from query_tracks with [sparse] overflight gps data to
        interpolate a spline fit on
    starttime (Timestamp): start time for the overflight (AK/local time)
    endtime (Timestamp): end time for the overflight (AK/local time)
    dT (float): optional, Time interval for the parametric spline fitting. Default 1.0 [s]
    out_crs (str): optional, output coordinate system for the densified flight. Note that interpolation necessarily
        occurs in UTM coordinates; frequent coordinate transformations may slow code. Default 'epsg:4326' (WGS84)
    plotting (bool): optional, creates a 3D plot of the flight trajectory and interpolated spline. Default False
    time_index (str): optional, time column for the flight tracks. Column should be type np.datetime64

    Outputs
    -------
    flight_spline (geodataframe): densified flight trajectory indexed by seconds after the initial point
    """

    flight_data = flight_utm.geometry.bounds.iloc[:, 0:2]  # retrieves the x and y of each point in UTM
    flight_times = (flight_utm[time_index] - starttime).dt.total_seconds().values  # seconds after initial point
    if len(flight_data) > 3:
        k = 3  # order of polynomial to fit to the spline; maximum cubic
    else:
        k = len(flight_data) - 1  # if there are less than 3 points, cannot fit a cubic spline. Choose lower order

    x = flight_data.values[:, 0]
    y = flight_data.values[:, 1]
    z = flight_utm.altitude_ft * 0.3048  # elevation [m]

    # parametric interpolation on the time interval provided
    duration = (endtime - starttime).total_seconds()
    tck, u = interpolate.splprep([x, y, z], u=flight_times, k=k)
    tnew = np.arange(0, duration + dT, dT)  # new spline fit with deltaT
    spl_out = interpolate.splev(tnew, tck)

    times_spline = [starttime + dt.timedelta(seconds=offset) for offset in tnew]
    flight_spline = gpd.GeoDataFrame(geometry=[Point(xyz) for xyz in zip(spl_out[0], spl_out[1], spl_out[2])],
                                     crs=flight_utm.crs, index=tnew)  # convert to a geodatabase

    if plotting:  # do this before (possibly) changing to WGS coordinates
        fig = plt.figure()
        ax3 = fig.add_subplot(projection='3d')
        ax3.plot(spl_out[0], spl_out[1], spl_out[2], color='deepskyblue', lw=1)
        ax3.scatter(x, y, z, marker='*', alpha=0.7, color='blue')
        # ax3.surf()

        # equal axes in 3D (sadly) do not exist yet, so here is a workaround:
        # https://stackoverflow.com/questions/13685386/matplotlib-equal-unit-length-with-equal-aspect-ratio-z-axis-is-not-equal-to
        max_range = np.array([x.max() - x.min(), y.max() - y.min(), z.max() - z.min()]).max() / 2.0

        mid_x = (x.max() + x.min()) * 0.5
        mid_y = (y.max() + y.min()) * 0.5
        ax3.set_xlim(mid_x - max_range, mid_x + max_range)
        ax3.set_ylim(mid_y - max_range, mid_y + max_range)
        # exaggerate z-direction by 10x and start at zero instead of centered around its midpoint
        ax3.set_zlim(0, 2 * max_range / 10)

        plt.show()

    if flight_spline.crs != out_crs:
        flight_spline = flight_spline.to_crs(out_crs)  # these conversions are slow

    # append useful rows from the original dataframe
    flight_spline["id"] = flight_utm["id"].iloc[0]  # broadcasts first row from flight_utm to all rows in flight_spline
    flight_spline["flight_id"] = flight_utm["flight_id"].iloc[0]
    flight_spline["registration"] = flight_utm["registration"].iloc[0]
    flight_spline["ak_datetime"] = times_spline

    return flight_spline


def get_time_delay(flight_utm, site_info, site_utm=None, M1=343.):
    """
    Calculates the 3D euclidean distance between the sound monitoring site and points on an aircraft trajectory in [m].
    Divides by the speed of sound (Mach 1; nominally 343 m/s) for a crude estimate to the time delay between sound
    being emitted by the aircraft and picked up by the microphone.

    Inputs
    ------
    flight_utm (geodataframe): Points (preferably in UTM) for an overflight, either queried from query_tracks or
        interpolated from interp_spline(). Must include elevation data in a column called "altitude_ft" or as a third
        dimension of the geometry. 3D point elevations are assumed to be in meters.
    site_info (dataframe): site metadata, including lat/long and elevation [m]
    site_utm (geodataframe): optional, contains UTM coordinates for the sound monitoring station. If not included, then
        this is gleaned from the site_info (deprecated)
    M1 (float): optional, speed of sound [m/s]. Default 343 m/s

    Outputs
    -------
    flight_utm (geodataframe): same as input with columns appended called 'time_delay' [s] and 'time_audible' [s]
    """
    # double check that the flight is in UTM coordinates:
    utm_zone = coord_to_UTM(flight_utm.iloc[0].geometry, crs=flight_utm.crs)
    if utm_zone != flight_utm.crs:
        flight_utm.to_crs(utm_zone, inplace=True)

    # iterate through flight points and calculate distance at each one
    if 'altitude_ft' in flight_utm.columns:
        altitude = np.multiply(flight_utm.altitude_ft.values, 0.3048)  # pull altitudes and scale ft to meters
    else:
        # assume these are in meters from interp_spline()
        altitude = flight_utm.geometry.z.values

        if np.any(np.isnan(altitude)):  # check: no altitude data found
            raise ValueError("No altitudes found for the given flight track")


    elev = site_info["elevation"].values[0]  # throws IndexError if no elevation is given

    if site_utm is None:
        try:
            site_utm = gpd.GeoDataFrame(geometry=[Point(site_info.x, site_info.y)], crs=site_info.utm_zone.values[0])
        except AttributeError as e:
            # no columns x, y, or utm_zone in site_utm. Hopefully we don't get here
            raise e

    # TODO: distance computation fails if no elevation data are provided
    dist = flight_utm.geometry.apply(lambda g: site_utm.geometry.distance(g))  # somehow this is a geodataframe... ?
    dist_2d = dist.values[:, 0]
    dist_3d = np.sqrt(np.square(dist_2d) + np.square((altitude - elev)))

    dist = pd.DataFrame({"distance": dist_3d}, index=dist.index)  # rewrite as dataframe
    flight_utm['time_delay'] = dist / M1
    flight_utm['time_audible'] = [row.ak_datetime + dt.timedelta(seconds=row.time_delay)
                                  for k, row in flight_utm.iterrows()]  # This is a timestamp object

    # short print statement
    nearest = flight_utm.loc[flight_utm.time_delay == flight_utm.time_delay.min()]  # find row for minimum distance
    min_dist = dist['distance'].min()
    min_delay = flight_utm['time_delay'].min()
    print(f"\tClosest point is at {min_dist:.0f} m, causing a time delay of {min_delay:.1f} s.")
    closest_time = (nearest.ak_datetime + dt.timedelta(seconds=min_delay)).array

    return flight_utm, closest_time


def compute_spline(flight_utm, site_info,
                   dT=1.0, plotting=False, time_index='ak_datetime', site_utm=None, M1=343.):
    """
    Calculates the 3D euclidean distance between the sound monitoring site and points on the spline fit of an aircraft
    trajectory in [m] to one second time resolution. Divides by the speed of sound (Mach 1; nominally 343 m/s) for a
    crude estimate to the time delay between sound being emitted by the aircraft and picked up by the microphone.

    Calls interp_spline() and get_time_delay()

    Inputs
    ------
    flight_utm (geodataframe): a Pandas dataframe from query_tracks with [sparse] overflight gps data to
        interpolate a spline fit on
    dT (float): optional, Time interval for the parametric spline fitting. Default 1.0 [s]
    plotting (bool): optional, creates a 3D plot of the flight trajectory and interpolated spline. Default False
    time_index (str): optional, time column for the flight tracks. Column should be type np.datetime64
    site_info (dataframe OR str): site metadata, including lat/long and elevation [m] OR a designator (UNITSITEYEAR)
    site_utm (geodataframe): optional, contains UTM coordinates for the sound monitoring station. If not included, then
        this is gleaned from the site_info (but conversions are slow so it is best to include)
    M1 (float): optional, speed of sound [m/s]. Default 343 m/s

    Outputs
    -------
    flight_spline (geodataframe): same as input with columns appended called 'time_delay' [s] and 'time_audible' [s]
    closest_time (DateTime): local time where the flight is closest to the site
    """

    # parse site_info
    if isinstance(site_info, str):
        u, s, yr = parse_designator(site_info)
        site_info = get_site_info(unit_name=u, site_name=s, year=yr)

    # double check that the flight is in UTM coordinates:
    utm_zone = site_info.utm_zone.values[0]
    if utm_zone != flight_utm.crs:
        flight_utm = flight_utm.to_crs(utm_zone)

    starttime = flight_utm.ak_datetime.iloc[0]
    endtime = flight_utm.ak_datetime.iloc[-1]

    spline = interp_spline(flight_utm, starttime, endtime,
                           dT=dT, out_crs=utm_zone, plotting=plotting, time_index=time_index)

    flight_spline, closest_time = get_time_delay(spline, site_info, site_utm=site_utm, M1=M1)
    return flight_spline, closest_time


def compute_intervals(flight_utm, site_info,
                      active_space=None, time_index='ak_datetime', dT=1., M1=343., min_interval=5):
    """
    Returns a list of audible intervals for a given flight and site location, factoring in time delay

    Parameters
    ----------
    flight_utm (geodataframe): a Pandas dataframe from query_tracks with [sparse] overflight gps data to
        interpolate a spline fit on
    site_info (dataframe OR str): site metadata, including lat/long and elevation [m] OR a designator (UNITSITEYEAR)
    active_space (GeoDataFrame): optional, active space of a given ground location. If left empty, calls
        get_active_space() to find the active space corresponding to the given flight.
    time_index (str): optional, time column for the flight tracks. Column should be type np.datetime64
    dT (float): optional, Time interval for the parametric spline fitting. Default 1.0 [s]
    M1 (float): optional, speed of sound [m/s]. Default 343 m/s
    min_interval (int): optional, minimum length of a time interval [s], either audible or inaudible. Default 5 [s]

    Returns
    -------
    intervals (list): a list of tuples corresponding to audible times (datetime_start, datetime_end)
    """

    if isinstance(site_info, str):  # if given a designator, turn this into a dataframe
        u, s, yr = parse_designator(site_info)
        site_info = get_site_info(unit_name=u, site_name=s, year=yr)

    # first, check that the flight is in UTM that matches the site
    utm_zone = site_info.utm_zone.values[0]
    if flight_utm.crs != utm_zone:
        flight_utm = flight_utm.to_crs(utm_zone)

    starttime = flight_utm.ak_datetime.iloc[0]
    endtime = flight_utm.ak_datetime.iloc[-1]

    spline = interp_spline(flight_utm, starttime, endtime, dT=dT, out_crs=utm_zone, time_index=time_index)

    # load the active space if it does not exist:
    if active_space is None:
        active_space = get_active_space(site_info.unit.values[0], site_info.code.values[0])

    # check to make sure the Active Space crs is the same as the flight... if not, change to match
    if active_space.crs != utm_zone:
        active_space.to_crs(utm_zone, inplace=True)

    # iterate through all spline points and check if they are in the active space
    in_AS_gdf = gpd.clip(spline, active_space)

    # make an `in_activespace` column and set to true for points inside mask
    spline['in_AS'] = False
    spline.loc[in_AS_gdf.index, 'in_AS'] = True  # matching by index

    # to detect where the crossings occur, use differencing
    # edges = 1 -> entering active space; edges = -1 -> leaving active space.
    edges = np.diff(spline.in_AS.astype(float).values[:])
    if spline.in_AS.iloc[0]:
        edges[0] = 1  # immediately entering active space, add first point as an 'edge'
    if spline.in_AS.iloc[-1]:
        edges[-1] = -1  # still in active space upon landing, add last point as an 'edge'

    indices = np.where(edges != 0)[0]
    if len(indices) == 0:
        return []  # returns an empty list if there are no audible intervals!

    spline_subset = spline.iloc[indices]
    spline_subset, _ = get_time_delay(spline_subset, site_info)  # compute the time delay

    # now we have audio record times corresponding to a flight's audiblity
    interval_len = np.diff(spline_subset.time_audible.values[:])  # for some reason, this is in [ns]
    min_interval = np.timedelta64(int(min_interval), 's')

    # if there are intervals longer than the minimum allowable, loop through and remove them
    filter = np.ones(len(indices))
    while np.any(min_interval > interval_len):
        # locate the earliest occurance of this:
        ind = np.where((min_interval > interval_len))[0][0]

        # add that index and the next one to filter out
        filter[ind:ind + 2] = 0
        interval_len[ind:ind + 2] = min_interval  # overwrite this interval as well as the next interval to move on

    # filter out values:
    filter = filter.astype(bool)
    indices = indices[filter]
    spline_subset = spline_subset.iloc[filter]

    aud_intervals = []

    for i, ind in enumerate(indices):
        # loop through edge indices
        if edges[ind] == 1:
            # default: closed on the right
            aud_intervals.append((spline_subset.time_audible.iloc[i],
                                  spline_subset.time_audible.iloc[i + 1]))
    return aud_intervals


def compute_f1(valid_points, act_space):
    """
    Given a set of annotated points and an active space geometry, compute accuracy metrics such as F1 score, precision,
    and recall.
        TP = True Positives
        FP = False Positives
        TN = True Negatives
        FN = False Negatives

    Inputs
    ------
    valid_points (geodataframe): must include geometry and an 'audible' column as annotated with trackbars
    act_space (geodataframe): Polygon or Multipolygon of the computed active space from ActiveSpace.py

    Returns
    -------
    f1 (float): f1 score (more here: https://en.wikipedia.org/wiki/F-score)
    precision (float): Defined TP/(TP+FP), measure of how well a positive test corresponds with an actual audible flight
    recall (float): Defined TP/(TP+FN), measure of how well an audible flight is marked as audible by the given active
        space
    n_tot (int): number of points annotated
    """

    # before computing anything, make sure projections match:
    if valid_points.crs != act_space.crs:
        act_space.to_crs(valid_points.crs, inplace=True)

    # iterate through all valid points and check if they are in the active space... this takes a while
    in_AS_gdf = gpd.clip(valid_points, act_space)

    # make an `in_activespace` column and set to true for points inside mask
    valid_points['in_AS'] = False
    valid_points.loc[in_AS_gdf.index, 'in_AS'] = True

    in_AS = valid_points.in_AS.values  # convert both of these columns to boolean arrays for easier
    audible = valid_points.audible.values

    # compute true positives, etc.
    TP = np.all([in_AS, audible], axis=0).sum()
    FP = np.all([in_AS, ~audible], axis=0).sum()
    FN = np.all([~in_AS, audible], axis=0).sum()
    TN = np.all([~in_AS, ~audible], axis=0).sum()
    n_tot = len(valid_points)

    precision = TP / (TP + FP)  # specificity... if a flight enters the active space, is it actually audible?
    recall = TP / (TP + FN)  # sensitivity... if a flight is audible, does it enter the active space?
    f1 = TP / (TP + 0.5 * (FP + FN))

    return f1, precision, recall, n_tot


def get_omni_sources(source_dir, n_sources=31, upper=None, lower=None):
    """
    Returns a dataframe of source file paths (and their respective gain gleaned from the filename) that are contained
    in a given directory. Omnidirectional NMSim source filenames should follow the naming convention:
        `O_SGGG.src` where S is a sign (+ or -) and GGG is the gain in centibels.

    Inputs
    ------
    source_dir (str): directory with [multiple] sound source files formatted in the name convention given above
    n_sources (int): number of sources to return. Sources are uniformly sampled from the entire set of sources in the
        given directory. If n_sources is larger than the number of source files in source_dir, return the full list of
        sources from source_dir

    Returns
    -------
    subset (dataframe): data frame with columns for the full file path (str) to each source and the gain (float)
    """

    filelist = os.listdir(source_dir)
    file_df = pd.DataFrame({'filename': filelist})
    filter1 = [name.startswith('O_') for name in filelist]  # filter out template files
    filter2 = [name.endswith('.src') for name in filelist]  # filters out .avg files

    file_df = file_df.loc[np.all([filter1, filter2], axis=0)]  # this will give only the .src files
    file_df.loc[:, 'full_path'] = source_dir + os.sep + file_df.filename

    # include column for gain, in dB (converted from cB)
    file_df.loc[:, 'gain'] = [float(gain.split('_')[1][:-4]) * 0.1 for gain in file_df.filename]
    # sort by gain amount
    file_df = file_df.sort_values('gain', ascending=False)

    # parse upper and lower limits
    if upper is None and lower is None:
        pass
    else:
        if upper is None:
            upper = file_df.gain.max()
        if lower is None:
            lower = file_df.gain.min()

        # filter on upper and lower bounds
        file_df = file_df.loc[np.all([file_df['gain'] <= upper, file_df['gain'] >= lower], axis=0)]

    # get subset of sources
    if n_sources < 0:
        warnings.warn('Number of sources must be greater than zero')
        n_sources = len(file_df)

    if n_sources < len(file_df):
        ind = np.linspace(0, len(file_df) - 1, n_sources, dtype='int')  # indices
        subset = file_df.iloc[ind]  # returns a subset
    else:
        subset = file_df  # the subset is the whole dataframe

    return subset


def yes_no_button(prompt=''):
    """
    Creates a yes/no selection window.

    Inputs
    ------
    prompt (str): Prompt to be displayed on the yes/no figure.

    Returns
    -------
    response (bool): True (yes) or False (no)
    """

    fig_button = plt.figure(figsize=(3, 2))
    # add radio buttons (selection) too:
    yesno_ax = plt.axes([0.25, 0.25, 0.5, 0.5])
    yesno_button = RadioButtons(yesno_ax, ['Yes', 'No'], active=None)
    plt.text(0.5, 1.1, prompt, horizontalalignment='center')
    to_return = None

    def yesno_clicked(event):
        # for radio buttons, `event` is a string of the label selected

        nonlocal to_return
        if event == "Yes":
            to_return = True
        else:
            to_return = False

        plt.close()

    yesno_button.on_clicked(yesno_clicked)
    plt.show()
    while to_return is None:
        plt.waitforbuttonpress()

    return to_return


def clip_DEM(coord, crs, out_path=None, proj_dir=None, side_len=50000, out_crs='epsg:4269',
             tmp_dir=None, overwrite=False,
             src_path=r"T:\ResMgmt\WAGS\Sound\Users\Kirby_Heck\NIMSIM_DEMSs\16_Bit\DENA_DEM_m_4269.TIF"):
    """
    Clips a square DEM from a park- or region-wide DEM.

    Parameters
    ----------
    coord (tuple): (x, y) or (long, lat) coordinate-tuple of the centroid
    crs (str): coordinate reference system corresponding to (x,y)
    out_path (str): optional, path to save the new DEM. Overrides `proj_dir` input
    proj_dir (str): optional, project directory containing `.\Input_Data\\01_ELEVATION\`, expected path for NMSim batch
        analysis DEM files. Either `out_path` or `proj_dir` are required inputs
    side_len (float): optional, size length of the new DEM in meters. Default 50 kms
    out_crs (str): optional, output coordinate projection for the DEM. Default 'epsg:4269' (NMSim default)
    tmp_dir (str): optional, directory path for a temporary intermediate DEM file(s)
    overwrite (bool): optional, include to (possibly) overwrite a prior DEM of the same name
    src_path (str): optional, file path to the source DEM

    Returns
    -------
    out_path (str): path that the DEM is written to
    """
    if tmp_dir is None:
        tmp_dir = tempfile.gettempdir()

    PCS = "epsg:6393"  # projected coordinate system, in meters. Needs to change with region; currently: AK Albers

    if out_path is None:
        zone = int(coord_to_UTM(coord, crs)[-2:])
        out_path = proj_dir + os.sep + r'Input_Data\01_ELEVATION\elevation_m_utm{:0d}.tif'.format(zone) \
 \
    # check to see if this file already exists:
    if os.path.exists(out_path) and not overwrite:
        return out_path

    # first, open the full AKR DEM and get the proper window
    with rasterio.open(src_path) as dem:
        pt = gpd.GeoDataFrame(geometry=[Point(coord)], crs=crs)

        # mark the extents as points by translating the given coordinate in a projected coordinate system
        extents = pt.to_crs(PCS).translate(xoff=-side_len // 2, yoff=side_len // 2)  # upper left
        extents = extents.append(pt.to_crs(PCS).translate(xoff=side_len // 2, yoff=-side_len // 2))  # lower right

        # change the extents to the DEM crs
        extents = extents.to_crs(dem.crs)

        # check if the given (x,y) crs is the same as the DEM crs
        # python can make this comparison even though `dem.crs` is a CRS object and `crs` is a string
        if dem.crs != crs:
            pt = pt.to_crs(dem.crs)
        x, y = pt.geometry.x, pt.geometry.y

        # create a window from these bounds
        lrow, lcol = dem.index(extents.iloc[0].x, extents.iloc[0].y)  # upper left is at the origin
        urow, ucol = dem.index(extents.iloc[1].x, extents.iloc[1].y)

        # check to make sure none of the bounds are negative:
        bnds = np.array([lrow, urow, lcol, ucol])
        if np.any(bnds < 0):
            bnds[bnds < 0] = 0
            lrow, urow, lcol, ucol = bnds

        window = Window.from_slices((lrow, urow), (lcol, ucol))

        # copy the raster data and metadata
        dem_data = dem.read(1, window=window)
        raster_meta = dem.profile

    # next, write a windowed area of `w` to a new DEM
    with rasterio.open(tmp_dir + os.sep + 'tmp1.tif', 'w', **raster_meta) as src:
        src.write(dem_data, window=window, indexes=1)

    # finally, open the windowed DEM and crop it to the window size (i.e. nodata size)
    with rasterio.open(tmp_dir + os.sep + 'tmp1.tif') as src:
        kwargs = src.meta.copy()
        kwargs.update({
            'height': window.height,
            'width': window.width,
            'transform': rasterio.windows.transform(window, src.transform)})

        # write to the output path
        if src.crs == out_crs:
            with rasterio.open(out_path, 'w', **kwargs) as dst:
                dst.write(src.read(window=window))

        else:
            with rasterio.open(tmp_dir + os.sep + 'tmp2.tif', 'w', **kwargs) as tmp:
                tmp.write(src.read(window=window))

            # reproject with gdal
            ds = gdal.Open(tmp_dir + os.sep + 'tmp2.tif')
            gdal.Warp(out_path, ds, dstSRS=out_crs)
            ds = None  # close file

            print('clip_DEM(): REPROJECTED WITH GDAL FROM ' + src.crs + ' TO ' + out_crs)

    return out_path


def create_NMSIM_flt(src, dst=None,
                     gdal_translate_path=r'T:\ResMgmt\WAGS\Sound\Users\Kirby_Heck\gdal\bin\gdal\apps\gdal_translate.exe'):
    """
    Takes a .tif GeoTIFF and uses gdal_translate.exe to write associated .flt (grid float raster) files.

    Parameters
    ----------
    src (str): .tif path
    dst (str): optional, destination location ending in .flt. Default: root directory of src with same filename
    gdal_translate_path (str): optional, path to look for gdal_translate.exe.

    Returns
    -------
    None (writes files to the root directory of src)
    """

    # no destination provided, use the same directory
    if dst is None:
        dst = os.path.splitext(src)[0] + '.flt'  # removes the .tif file extension and appends '.flt'

    # gdal_translate.exe command line args; `ot` output type, `of` output format `a_nodata` no-data filler
    cmd = "-ot Float32 -of ehdr -a_nodata -9999"

    def quote(item):
        # this avoids any issues with file paths including white space as command line arguments
        return "\"" + item + "\""

    fullCmd = ' '.join([gdal_translate_path, cmd, quote(src), quote(dst)])
    subprocess.call(fullCmd)  # call GDAL

    # the header file doesn't write correctly... manually overwrite this:
    hdr = os.path.splitext(src)[0] + '.hdr'  # removes the .tif file extension and appends '.hdr'
    old_hdr = pd.read_csv(hdr, header=None, delim_whitespace=True, index_col=0).T

    # compute new lower left corner y-val
    yllcorner = float(old_hdr.ULYMAP) - float(old_hdr.NROWS) * float(old_hdr.XDIM)

    # write a new header file exactly as output by ESRI
    with open(hdr, 'w') as header:
        header.write("{:14}{:}\n".format("ncols", old_hdr.NCOLS.values[0]))
        header.write("{:14}{:}\n".format("nrows", old_hdr.NROWS.values[0]))
        header.write("{:14}{:}\n".format("xllcorner", old_hdr.ULXMAP.values[0]))
        header.write("{:14}{:}\n".format("yllcorner", yllcorner))
        header.write("{:14}{:}\n".format("cellsize", old_hdr.XDIM.values[0]))
        header.write("{:14}{:}\n".format("NODATA_value", old_hdr.NODATA.values[0]))
        header.write("{:14}{:}".format("byteorder", "LSBFIRST"))


def dem_ft2m(src, dst=None):
    """
    Takes a DEM GeoTIFF in feet and saves a copy of the DEM in meters.

    Parameters
    ----------
    src (str): .tif path
    dst (str): optional, destination to save new DEM (file extension should be .tif). See src in create_NMSIM_flt()

    Returns
    -------
    None, writes a .tif file and accompanying .flt files (see create_NMSIM_flt)
    """

    # first, open the full AKR DEM and get the proper window
    if dst is None:
        # destination path begins with 'elevation_m' (ideally, replacing 'elevation_') in the source
        dst = os.path.dirname(src) + os.sep + 'elevation_m_' + os.path.basename(src)[10:]

    # first, open the original DEM [in ft]
    with rasterio.open(src) as dem:
        # copy the raster data into `raster_data`
        raster_data = dem.read(1)
        raster_meta = dem.profile

    # compute the new data (scale raster by ft-to-m conversion, but omit nodata values) and write to new dem
    with rasterio.open(dst, 'w', **raster_meta) as src:
        new_data = raster_data
        new_data[raster_data != dem.nodata] = raster_data[raster_data != dem.nodata].astype(np.float64) * 0.3048
        src.write(new_data.astype(np.int16), indexes=1)

        print('dem_ft_to_m(): done writing to', dst)

    # create .flt format of the same (new) DEM
    create_NMSIM_flt(dst)


def get_active_space(u, s, root_dir=r'T:\ResMgmt\WAGS\Sound\Users\Kirby_Heck\NMSIM_ProjectDirectories',
                     search_str='*_active_spaces_Mennitt_*ft.txt', index='gain', out_crs=None, all=False):
    """
    Looks through truthing results and returns a GeoDataFrame of the active space with the highest F1 score for a site.

    Parameters
    ----------
    unit_name (str): Four-character NPS unit name
    site_name (str): Four-character (does this work with more than four characters?) site name
    root_dir (str): optional, path to the root of all project directories.
    search_str (str): optional, text file name to search for within the project directory. Default is
        '*_active_spaces_Mennitt_*ft.txt'
    index (str): optional, searches for a column with the given string to mark as the index
    out_crs (str): optional, output CRS. Default is the utm zone of the site.
    all (bool): optional, returns ALL of the truthing active spaces instead of just the highest F1 score. Default False

    Returns
    -------
    active_space (GeoDataFrame)
    """

    # get the UTM zone from the designator as well:
    site_info = get_site_info(unit_name=u, site_name=s)
    utm_zone = site_info.utm_zone.values[0]

    results_path = glob.glob(root_dir + os.sep + u + s + os.sep + search_str)[0]

    res = pd.read_csv(results_path)
    res['geometry'] = gpd.GeoSeries.from_wkt(res['geometry'])
    results = gpd.GeoDataFrame(res, geometry='geometry', crs=utm_zone)
    if out_crs is not None:
        results.to_crs(out_crs, inplace=True)  # reproject

    try:
        results = results.set_index(index)
    except KeyError:
        pass  # `index` is not a column in `results`

    if all:
        return results
    else:
        active_space = results.loc[results.f1 == results.f1.max()]

    return active_space


def make(path):
    """
    Safely create a folder. Copied from NMSIM_Create_Base_Layers.py
    """

    if not os.path.exists(path):
        os.makedirs(path)


def make_NMSIM_project_dir(project_dir):
    """
    Create a canonical NMSIM project directory. Copied from NMSIM_Create_Base_Layers.py

    Parameters
    ----------
    project_dir (str): a path location where an NMSIM project directory will be created
    """

    # a list of all the subfolders for a project
    subfolders = [r"Input_Data", r"Input_Data\01_ELEVATION", r"Input_Data\02_IMPEDANCE", r"Input_Data\03_TRAJECTORY",
                  r"Input_Data\04_LAYERS", r"Input_Data\05_SITES", r"Input_Data\06_AMBIENCE", r"Input_Data\07_WEATHER",
                  r"Input_Data\08_TREES", r"Output_Data", r"Output_Data\ASCII", r"Output_Data\IMAGES",
                  r"Output_Data\SITE",
                  r"Output_Data\TIG_TIS"]

    # make all the subfolders
    for folderExt in subfolders:
        make(project_dir + os.sep + folderExt)


def create_blank_raster(poly, out_path,
                        cellsize=1000, geotransform=None, projection=None,
                        driver='GTiff', etype=gdal.GDT_Float32, nodata=-999., overwrite=False):
    """
    Creates a raster of the polygon and writes the file to the provided output path.

    Parameters
    ----------
    poly (shapely polygon): a shapely polygon to rasterize; must match given projection and geotransform
    out_path (str): filename of output raster; must match with given driver
    cellsize (int): optional, cell size [m]. Default 1000 m
    geotransform (str): optional, given geotransform for the output raster. Default is calculated from the input polygon
    projection (str): optional, map projection given by GDAL (e.g. output of `source.GetGeoTransform()`). Default is
        the GDAL equivalent of Alaska Albers, EPSG:6393
    driver (str): optional, driver for writing the raster. Default is GeoTIFF
    etype (GDT types): optional, GDAL Data Type for writing the raster. Default is gdal.GDT_Float32
    nodata (dtype): optional, nodata value for the raster. Default -999.
    overwrite (bool): optional, overwrites an existing raster. Default False

    Returns
    -------
    coord_key (dataframe): pandas dataframe that includes entries for the ID number, row, column, and (x,y) location of
        each point in the raster. This file is also written to the same directory as the saved raster (`out_path`)
    Saves a raster to `outpath`
    """

    if os.path.exists(out_path) and not overwrite:
        warnings.warn("File already exists at " + out_path + ", returning coordinate key.")
        return get_coord_key(out_path, write=False)

    if projection is None:
        # there is a reason this behemoth didn't go in the function definition
        # taken from the AKR DEM file
        projection = 'PROJCS["NAD_1983_Albers",GEOGCS["NAD83",DATUM["North_American_Datum_1983",\
        SPHEROID["GRS 1980",6378137,298.257222101004,AUTHORITY["EPSG","7019"]],\
        AUTHORITY["EPSG","6269"]],PRIMEM["Greenwich",0],UNIT["degree",0.0174532925199433,AUTHORITY["EPSG","9122"]],\
        AUTHORITY["EPSG","4269"]],PROJECTION["Albers_Conic_Equal_Area"],PARAMETER["latitude_of_center",50],\
        PARAMETER["longitude_of_center",-154],PARAMETER["standard_parallel_1",55],PARAMETER["standard_parallel_2",65],\
        PARAMETER["false_easting",0],PARAMETER["false_northing",0],UNIT["metre",1,AUTHORITY["EPSG","9001"]],\
        AXIS["Easting",EAST],AXIS["Northing",NORTH]]'

    # using the polygon and cell size, find the extents and output raster size
    bounds = poly.bounds
    geotransform = (bounds.minx.values[0], cellsize, 0, bounds.maxy.values[0], 0, -cellsize)

    xsize = int(np.ceil((bounds.maxx - bounds.minx).values[0] / cellsize))
    ysize = int(np.ceil((bounds.maxy - bounds.miny).values[0] / cellsize))

    driver_gdal = gdal.GetDriverByName(driver)

    data = np.zeros((ysize, xsize))  # this is all zeros
    new_ds = driver_gdal.Create(out_path,
                                xsize=xsize, ysize=ysize,
                                bands=1,
                                eType=etype)

    # set projection
    new_ds.SetGeoTransform(geotransform)
    new_ds.SetProjection(projection)
    new_ds.GetRasterBand(1).WriteArray(data)
    new_ds.GetRasterBand(1).SetNoDataValue(nodata)
    del (new_ds)  # close file

    # now, apply a mask in the shape of the polygon:
    with rasterio.open(out_path) as dst:
        out_image, _ = rasterio.mask.mask(dst, poly)
        out_meta = dst.meta.copy()

    # overwrite the completely blank raster with the mask applied
    with rasterio.open(out_path, "w", **out_meta) as dst:
        dst.write(out_image)
    print("create_blank_raster(): Blank raster saved successfully in " + out_path)

    # ============ The raster is now written, but the coordinate key is not. Calls get_key() =============

    coord_key = get_coord_key(out_path, write=True)
    return coord_key


def get_coord_key(raster_path, write=False):
    """
    Retrieves or creates a coordinate-matching key for the target raster.

    Parameters
    ----------
    raster_path (str): path to search for target raster
    write (bool): optional, writes the returned dataframe to a .csv True. Default False

    Returns
    -------
    key (dataframe): a dataframe that matches coordinates (x, y) to the rows and columns of a raster. Also assigns an
        ID number to each point that contains data.
    """

    # try to find the file if a key was passed in
    if raster_path.endswith('_key.csv'):
        key_path = raster_path
    else:  # or make a new one from the corresponding .tif filename
        key_path = os.path.splitext(raster_path)[0] + '_key.csv'

    # creates a new file called `DIR\[raster_name]_key.csv` if one does not exist
    if not write:  # try to load the key directly
        try:
            key = pd.read_csv(key_path)
            key = key.set_index('id')  # set index
            return key
        except FileNotFoundError as e:
            pass  # file does not yet exist

    with rasterio.open(raster_path) as src:
        minx, maxy = src.xy(0, 0)  # this is the upper-left location of the upper-left-most pixel
        filter = src.read(1) != src.nodata
        rows, cols = filter.shape

        # build the x-y grid:
        print(src.get_transform())
        geot = src.get_transform()
        cellsize = geot[1]

    # create the `key` to match (row, col) with (x, y)
    x = np.arange(minx, minx + cellsize * cols, cellsize)
    y = np.arange(maxy, maxy - cellsize * rows, -cellsize)
    X, Y = np.meshgrid(x, y)
    X[X * filter == 0] = np.nan  # set nodata values to np.nan
    Y[Y * filter == 0] = np.nan

    R, C = np.mgrid[0:rows, 0:cols]
    index = np.cumsum(~np.isnan(X.ravel()))  # number indices that are not np.nan
    index[np.isnan(X.ravel())] = 0  # set all duplicate indices to zero

    key = pd.DataFrame({'row': R.ravel(),
                        'col': C.ravel(),
                        'x': X.ravel(),
                        'y': Y.ravel()}, index=index)
    key.index.name = 'id'

    if write:
        key.to_csv(key_path)
        print('Coordinates key written to ' + key_path)

    return key


def get_coord_list(coord_key, filter_by='geometry'):
    """
    Parses a coordinate key and returns a list of coordinate pairs (x, y) in the projection of the source raster.

    Parameters
    ----------
    coord_key (dataframe): coordinate key as given by get_coord_key(). A string (path to the source file `*_key.csv`)
        can also be passed to coord_key, which will call get_coord_key() first.
    filter_by (str): column to filter the complete key by to yield only rows that have not been computed yet
        (i.e. isnan)

    Returns
    -------
    coords (list): List of length 2 lists: [id, (x, y)]
    """

    if isinstance(coord_key, str):
        # try to load from a filepath, may throw FileNotFoundError
        coord_key = get_coord_key(coord_key)

    # try to drop nodata entries (i.e. raster points outside of the park boundary)
    try:
        coord_key.drop(index=0, inplace=True)
    except KeyError:
        pass  # no points to drop

    # if we're loading results from an incomplete run, filter only for when the geometry is None
    if filter_by in coord_key.columns:
        coord_key = coord_key.loc[pd.isnull(coord_key[filter_by])]

    coords = [[row[1].name, (row[1].x, row[1].y)] for row in coord_key.iterrows()]
    return coords


def get_run_results(file_path, index='id'):
    """
    Reads a saved coordinate key from the provided filepath and returns a geodataframe of results.

    Parameters
    ----------
    file_path (str): file to look for written results
    index (str): optional, column to set as the index. Default 'id'

    Returns
    -------
    GeoDataFrame with results and any additional columns appended from metric calculations
    """

    # assume the file exists, throws FileNotFoundError otherwise
    data = pd.read_csv(file_path)

    # convert the geometry column from wkt strings to Shapely geometry objects
    data.loc[~pd.isnull(data.geometry), 'geometry'] = gpd.GeoSeries.from_wkt(
        data.loc[~pd.isnull(data.geometry), 'geometry'])

    res = gpd.GeoDataFrame(data, geometry=data.geometry, crs=data.crs.values[0])
    res = res.set_index(index)
    return res


def refresh_site_data(project_dir):
    """
    Refreshes a site by deleting contents in 01_ELEVATION and 05_SITES. (03_TRACKS overwrites itself)
        NOTE: Deprecated; use temp directory now.

    Parameters
    ----------
    project_dir (str): project directory to serach for sub-directories

    Returns None
    """

    site_dir = os.path.join(project_dir, 'Input_Data', '05_SITES')
    for f in os.listdir(site_dir):
        os.remove(os.path.join(site_dir, f))

    elev_dir = os.path.join(project_dir, 'Input_Data', '01_ELEVATION')
    for f in os.listdir(elev_dir):
        os.remove(os.path.join(elev_dir, f))


def get_raster_extent(ras_path, out_crs=None):
    """
    Loads a raster from ras_path with rasterio and returns a geodataframe of the bounding box.

    Parameters
    ----------
    ras_path (str): path to the raster file
    out_crs (str): optional, coordinate projection to return the polygon in. Default is the original projection of
        the raster file

    Returns
    -------
    mask (geodataframe): Geodataframe of a rectangle of the geometry of the raster bounds
    """

    with rasterio.open(ras_path) as src:
        bounds = src.bounds
        crs = src.crs

    poly = Polygon([(bounds.left, bounds.bottom),
                    (bounds.right, bounds.bottom),
                    (bounds.right, bounds.top),
                    (bounds.left, bounds.top)])
    mask = gpd.GeoDataFrame(geometry=[poly], crs=crs)
    # for some reason, query_tracks needs a 'dissolve_field' field and an additional field so `geometry` can be queried
    mask = mask.assign(dissolve_field=1, index=0)

    if out_crs is not None and crs != out_crs:
        mask = mask.to_crs(out_crs)

    return mask
